{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3e454a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from IPython.display import display\n",
    "import pandas as pd\n",
    "\n",
    "nlp = spacy.load(\"fr_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b60a379c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre de token:  1890\n"
     ]
    }
   ],
   "source": [
    "# Soit tout le texte d'un coup\n",
    "fulltext = \"\"\n",
    "with open(\"text1.txt\", \"r\", encoding=\"utf-8\") as text1:\n",
    "    fulltext = text1.read()\n",
    "\n",
    "\n",
    "doc = nlp(fulltext)\n",
    "\n",
    "print(\"Nombre de token: \", len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0c746f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graphe syntaxique de la première phrase du text1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"fr\" id=\"dd5280111b974468beac18784d0357ff-0\" class=\"displacy\" width=\"1450\" height=\"487.0\" direction=\"ltr\" style=\"max-width: none; height: 487.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Longtemps,</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">ADV</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">je</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">me</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">PRON</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">suis</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">AUX</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">couché</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">de</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">ADP</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">bonne</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"397.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">heure.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd5280111b974468beac18784d0357ff-0-0\" stroke-width=\"2px\" d=\"M70,352.0 C70,2.0 750.0,2.0 750.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd5280111b974468beac18784d0357ff-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,354.0 L62,342.0 78,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd5280111b974468beac18784d0357ff-0-1\" stroke-width=\"2px\" d=\"M245,352.0 C245,89.5 745.0,89.5 745.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd5280111b974468beac18784d0357ff-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,354.0 L237,342.0 253,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd5280111b974468beac18784d0357ff-0-2\" stroke-width=\"2px\" d=\"M420,352.0 C420,177.0 740.0,177.0 740.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd5280111b974468beac18784d0357ff-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">iobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M420,354.0 L412,342.0 428,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd5280111b974468beac18784d0357ff-0-3\" stroke-width=\"2px\" d=\"M595,352.0 C595,264.5 735.0,264.5 735.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd5280111b974468beac18784d0357ff-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux:tense</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M595,354.0 L587,342.0 603,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd5280111b974468beac18784d0357ff-0-4\" stroke-width=\"2px\" d=\"M945,352.0 C945,177.0 1265.0,177.0 1265.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd5280111b974468beac18784d0357ff-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">case</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,354.0 L937,342.0 953,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd5280111b974468beac18784d0357ff-0-5\" stroke-width=\"2px\" d=\"M1120,352.0 C1120,264.5 1260.0,264.5 1260.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd5280111b974468beac18784d0357ff-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,354.0 L1112,342.0 1128,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-dd5280111b974468beac18784d0357ff-0-6\" stroke-width=\"2px\" d=\"M770,352.0 C770,89.5 1270.0,89.5 1270.0,352.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-dd5280111b974468beac18784d0357ff-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">obl:mod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1270.0,354.0 L1278.0,342.0 1262.0,342.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"Graphe syntaxique de la première phrase du text1\")\n",
    "displacy.render(doc[:10], style=\"dep\", jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26004a03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Troisième token:  couché (couché), index:  5\n",
      "Voisins:  [Longtemps, ,, je, me, suis] [heure, .]\n",
      "Phrase:  Longtemps, je me suis couché de bonne heure.\n",
      "Edges:  Longtemps .\n",
      "Entity type:  0\n",
      "Lemma:  10940960452429598961 coucher\n",
      "Norm:  11184698726792784299 couché\n",
      "Shape:  13110060611322374290 xxxx\n",
      "Tag:  100 VERB\n",
      "Dep:  8206900633647566924 ROOT\n",
      "Sentiment:  0.0\n",
      "-----\n",
      "Troisième token:  Longtemps (Longtemps), index:  0\n",
      "Voisins:  [] []\n",
      "Phrase:  Longtemps, je me suis couché de bonne heure.\n",
      "Edges:  Longtemps Longtemps\n",
      "Entity type:  0\n",
      "Lemma:  10169700273540668978 longtemps\n",
      "Norm:  10169700273540668978 longtemps\n",
      "Shape:  16072095006890171862 Xxxxx\n",
      "Tag:  86 ADV\n",
      "Dep:  400 advmod\n",
      "Sentiment:  0.0\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "def print_token_info(token):\n",
    "    print(\"Troisième token: \", token, f\"({token.text}), index: \", token.i)\n",
    "    print(\"Voisins: \", list(token.lefts), list(token.rights))\n",
    "\n",
    "    print(\"Phrase: \", token.sent)\n",
    "    print(\"Edges: \", token.left_edge, token.right_edge)\n",
    "    print(\"Entity type: \", token.ent_type)\n",
    "    print(\"Lemma: \", token.lemma, token.lemma_)\n",
    "    print(\"Norm: \", token.norm, token.norm_)\n",
    "    print(\"Shape: \", token.shape, token.shape_)\n",
    "    print(\"Tag: \", token.tag, token.tag_)\n",
    "    print(\"Dep: \", token.dep, token.dep_)\n",
    "    print(\"Sentiment: \", token.sentiment)\n",
    "    print(\"-----\")\n",
    "\n",
    "\n",
    "print_token_info(doc[5])\n",
    "# print_token_info(doc[2])\n",
    "# print_token_info(doc[7])\n",
    "print_token_info(doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c857d04",
   "metadata": {},
   "source": [
    "Insérer des données dans Neo4j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0d0df37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from neo4j import GraphDatabase, Session\n",
    "from typing import Callable, Any\n",
    "\n",
    "URI = \"neo4j://localhost:7687\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c24d76e",
   "metadata": {},
   "source": [
    "Prise en main de Neo4j [Tutoriel Neo4j](https://neo4j.com/docs/python-manual/current/query-simple/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0cd2d4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(URI)\n",
    "tutorial_db = \"tutorial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0038bc44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes created:  2\n"
     ]
    }
   ],
   "source": [
    "insert = driver.execute_query(\"\"\"\n",
    "    CREATE (m:Monster {name: $monster})\n",
    "    CREATE (b:Person {name: $name})\n",
    "    CREATE (m)-[:ATE]->(b)\n",
    "\"\"\", monster=\"Chopacabra\", name=\"Cedric\", database=tutorial_db).summary\n",
    "\n",
    "print(\"Nodes created: \", insert.counters.nodes_created)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21462667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Monster Chopacabra ate Cedric\n"
     ]
    }
   ],
   "source": [
    "records, summary, keys = driver.execute_query(\n",
    "    \"MATCH (m:Monster)-[:ATE]->(p:Person) RETURN m.name as monster_name, p.name as person_eaten\",\n",
    "    database=tutorial_db,\n",
    ")\n",
    "\n",
    "for record in records:\n",
    "    data = record.data()\n",
    "    print(f'{data[\"monster_name\"]} ate {data[\"person_eaten\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "700d2f5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noeuds supprimés:  1 []\n"
     ]
    }
   ],
   "source": [
    "delete = driver.execute_query(\n",
    "    \"MATCH (a) DETACH DELETE a\",\n",
    "    database=tutorial_db,\n",
    ")\n",
    "\n",
    "print(\"Noeuds supprimés: \", delete.summary.counters.nodes_deleted, delete.records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b431bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a18882a",
   "metadata": {},
   "source": [
    "Inserer les données du graphe de spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "21ae5bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "database = \"neo4j\"\n",
    "driver = GraphDatabase.driver(URI)\n",
    "session = driver.session(database=database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7d7b6950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "417 noeuds et 466 liens supprimés\n"
     ]
    }
   ],
   "source": [
    "# Cleanup\n",
    "summary = driver.execute_query(\"Match (t: Token) DETACH DELETE t\").summary\n",
    "print(\n",
    "    f\"{summary.counters.nodes_deleted} noeuds et {summary.counters.relationships_deleted} liens supprimés\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef8915bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "412 noeuds et 455 liens crées\n"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.dep_ == \"ROOT\":\n",
    "        continue\n",
    "    if token.is_punct or token.head.is_punct:\n",
    "        continue\n",
    "    if token.is_space or token.head.is_space:\n",
    "        continue\n",
    "    if token.is_stop or token.head.is_stop:\n",
    "        continue\n",
    "    if token.is_quote or token.head.is_quote:\n",
    "        continue\n",
    "\n",
    "    data.append(\n",
    "        {\n",
    "            \"self\": {\"lemma\": token.lemma_, \"pos\": token.pos_},\n",
    "            \"head\": {\n",
    "                \"lemma\": token.head.lemma_,\n",
    "                \"pos\": token.head.pos_,\n",
    "            },\n",
    "            \"relationship\": token.dep_,\n",
    "        }\n",
    "    )\n",
    "\n",
    "query = \"\"\"\n",
    "    UNWIND $tokens as token\n",
    "    MERGE (self: Token { lemma: token.self.lemma, pos: token.self.pos })\n",
    "    MERGE (head: Token { lemma: token.head.lemma, pos: token.head.pos })\n",
    "    CREATE (self)-[:DEP { label: token.relationship }]->(head)\n",
    "\"\"\"\n",
    "\n",
    "result = session.execute_write(lambda tx: tx.run(query, tokens=data).to_eager_result())  # type: ignore\n",
    "\n",
    "print(\n",
    "    f\"{result.summary.counters.nodes_created} noeuds et {result.summary.counters.relationships_created} liens crées\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "593db860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sept', 'sera', 'était', 'on', 'via', 'hé', 'anterieure', 'facon', 'plus', 'différent', 'quelle', 'juste', 'dedans', 'son', 'ni', 'outre', 'antérieur', 'diverse', 'avant', 'sous', 'une', 'également', 'onze', 'lesquels', 'hors', 'desquelles', 'quatre', 'lorsque', 'soixante', 'vont', 'ouste', 'chacun', 'que', 'treize', 'apres', 'seulement', 'spécifiques', 'comment', 'auront', 'peux', 'certain', 'aux', 'hi', 'chez', 'qui', 'egalement', 'houp', 'cette', 'ceci', 'partant', \"c'\", 'durant', 'toi', 'ouias', 'possibles', 'six', 'dits', 'touchant', 'voilà', 'premièrement', 'a', 'avaient', 'là', 'quarante', 'autres', 'donc', 'plutôt', 'nombreuses', 'sur', 'longtemps', 'deuxième', 'mienne', 'dit', 'et', 'n’', 'étais', 'plutot', 'car', 'celle', \"n'\", 'pourrais', 'mien', 'pourrait', 'cet', 'dans', 'ô', 'lors', 'auxquels', 'dessus', 'autre', 'elles', 'celle-ci', 'hormis', 'ont', 'ceux-ci', 'vé', 'parlent', 'ouvert', 'quelles', 'envers', 'ces', 'douze', 'aurait', 'mes', 'quoi', 'première', 'aupres', 'abord', 'memes', 'certains', 'na', 'tiens', 'devers', 'pas', 'merci', 'sa', 'désormais', 'unes', 'te', 'dix-neuf', 'tels', 'suffisant', 'faisaient', 'etant', 'peut', 'plusieurs', 'desormais', 'deja', 'ouverte', 'c’', \"j'\", 'suis', 'être', 'sans', 'à', 'lui-même', 'i', 'â', 'un', 'suivant', 'cinquantième', 'pour', 'attendu', 'ne', 'parfois', 'voila', 'avait', 'deuxièmement', 'hui', 'mêmes', 'nous', 'nul', 'lui', 'seront', 'du', 'quiconque', 'differents', 'le', 'semblent', 'bat', 't’', 'proche', 'ait', 'restant', 'étaient', 'lesquelles', 'peuvent', 'celui', 'tres', \"t'\", 'entre', 'nous-mêmes', 'telles', 'vu', 'devra', 'quant', 'ouverts', 'serait', 'huit', 'avoir', 'prealable', 'vôtre', 'lequel', 'certes', 's’', 'sent', 'eu', 'il', 'sien', 'specifiques', 'ainsi', 'seules', 'différentes', 'quand', 'votres', 'tente', 'quatorze', 'la', 'seul', 'ton', 'moi-même', 'avons', 'compris', 'pres', 'tu', 'après', 'revoila', 'possible', 'dix', 'notamment', 'leur', \"l'\", 'quatrième', 'me', 'tout', 'seule', 'tend', 'neanmoins', 'malgre', \"m'\", 'procedant', 'vingt', 'etaient', 'cinquantaine', 'uns', 'en', 'nouveau', 'deux', 'restent', 'miens', 'quatre-vingt', 'specifique', 'nôtres', 'onzième', 'soi-même', 'retour', 'sauf', 'ou', 'cent', 'delà', 'vers', 'puisque', 'quant-à-soi', 'mon', \"s'\", 'tenant', 'votre', 'tes', 'celui-ci', 'cinquième', 'm’', 'font', 'aura', 'troisième', 'se', 'bas', 'moindres', 'dès', 'hue', 'es', 'suffit', 'depuis', 'permet', 'allons', 'siens', 'y', 'duquel', 'quelconque', 'vas', 'relativement', 'tellement', 'reste', 'surtout', 'seuls', 'tiennes', 'ho', 'importe', 'toute', 'quoique', 'antérieure', 'afin', 'maintenant', 'celui-la', 'ils', 'aie', \"quelqu'un\", 'où', 'cinquante', 'auquel', 'aussi', 'ha', 'celles', 'voici', 'vôtres', 'néanmoins', 'quelque', 'exactement', 'moins', 'parler', 'anterieures', 'elles-memes', 'hou', 'déja', 'pense', 'allaient', 'les', 'telle', 'lui-meme', 'suivante', 'suivantes', 'etc', 'divers', 'ceux-là', 'hem', 'peu', 'parmi', 'si', 'toutes', 'differente', 'anterieur', \"qu'\", 'celle-là', 'trente', 'alors', 'quel', 'sont', 'soit', 'feront', 'combien', 'différents', 'faisant', 'nombreux', 'quelques', 'sienne', 'miennes', 'certaine', 'cinq', 'tenir', 'trois', 'de', 'revoilà', 'sinon', 'huitième', 'semblaient', 'mais', 'va', 'elle', 'different', 'quels', 'maint', 'celles-la', 'nos', 'excepté', 'je', 'neuvième', 'ès', 'concernant', 'precisement', 'pourquoi', 'dire', 'etais', 'tel', 'étant', 'notre', 'as', 'leurs', 'ayant', 'semble', 'encore', 'lès', 'etre', 'sait', 'souvent', 'suivre', 'etait', 'personne', 'elle-même', 'o', 'qu’', 'rend', 'assez', 'malgré', 'debout', 'tienne', 'relative', 'hep', 'directe', 'près', 'rendre', 'toi-même', 'très', 'avais', 'semblable', 'celui-là', 'effet', 'jusque', 'suivants', 'or', 'elles-mêmes', \"d'\", 'au', 'suffisante', 'té', 'chaque', 'seize', 'ta', 'differentes', 'doit', 'ce', 'auxquelles', 'quinze', 'derrière', 'tien', 'troisièmement', 'vous', 'spécifique', 'est', 'dessous', 'toujours', 'selon', 'ci', 'parle', 'basee', 'celles-là', 'siennes', 'revoici', 'nôtre', 'vos', 'dixième', 'enfin', 'déjà', 'avec', 'vais', 'dejà', 'sixième', 'desquels', 'soi', 'doivent', 'ça', 'même', 'da', 'ah', 'mille', 'antérieures', 'derriere', 'dix-sept', 'premier', 'des', 'moi', 'précisement', 'quatrièmement', 'eh', 'laisser', 'chacune', 'ai', 'fait', 'ma', 'auraient', 'devant', 'jusqu', 'vous-mêmes', 'l’', 'tant', 'par', 'celle-la', 'fais', 'autrement', 'd’', 'préalable', 'elle-meme', 'seraient', 'autrui', 'suit', 'différente', 'gens', 'ceux', 'diverses', 'j’', 'certaines', 'tous', 'meme', 'cependant', 'comme', 'toi-meme', 'cela', 'dix-huit', 'environ', 'pouvait', 'stop', 'douzième', 'directement', 'moi-meme', 'parce', 'dite', 'soi-meme', 'laquelle', 'pendant', 'eux', 'celles-ci', 'façon', 'dehors', 'ses', 'eux-mêmes', 'puis', 'dont', 'pu', 'septième'}\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS\n",
    "print(STOP_WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be72ed7c",
   "metadata": {},
   "source": [
    "Lister tous les verbes du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51425151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119 verbes trouvés: \n",
      "chercher\n",
      "éveiller\n",
      "poser\n",
      "croire\n",
      "faire\n",
      "lire\n",
      "peser\n",
      "devenir\n",
      "étonner\n",
      "trouver\n",
      "doux\n",
      "apparaître\n",
      "éloigner\n",
      "relever\n",
      "décrire\n",
      "étendre\n",
      "hâter\n",
      "suivre\n",
      "oreiller\n",
      "regarder\n",
      "obliger\n",
      "coucher\n",
      "inconnu\n",
      "apercevoir\n",
      "lever\n",
      "sonner\n",
      "porter\n",
      "donne\n",
      "souffrir\n",
      "entendre\n",
      "porte\n",
      "éteindre\n",
      "falloir\n",
      "rester\n",
      "remède\n",
      "rendormir\n",
      "plonger\n",
      "retourner\n",
      "unir\n",
      "dissiper\n",
      "couper\n",
      "naître\n",
      "sentir\n",
      "baiser\n",
      "courbaturer\n",
      "donner\n",
      "retrouver\n",
      "voir\n",
      "désirer\n",
      "imaginer\n",
      "reculer\n",
      "savoir\n",
      "venir\n",
      "asseoir\n",
      "voyager\n",
      "ouvrir\n",
      "détendre\n",
      "lâcher\n",
      "tirer\n",
      "agiter\n",
      "réussir\n",
      "tourner\n",
      "remuer\n",
      "repérer\n",
      "induire\n",
      "reconstruire\n",
      "nommer\n",
      "changer\n",
      "hésiter\n",
      "identifier\n",
      "rappeler\n",
      "deviner\n",
      "allonger\n",
      "finir\n",
      "reposer\n",
      "devoir\n",
      "oublier\n",
      "suspendre\n",
      "figurer\n",
      "représenter\n",
      "revoir\n",
      "fermer\n",
      "vouloir\n",
      "souffler\n",
      "cesser\n",
      "prendre\n",
      "parler\n",
      "survivre\n",
      "choquer\n",
      "commencer\n",
      "détacher\n",
      "appliquer\n",
      "recouvrir\n",
      "demander\n",
      "graver\n",
      "réveiller\n",
      "réjouir\n",
      "pouvoir\n",
      "soulager\n",
      "disparaître\n",
      "partir\n",
      "révolu\n",
      "échapper\n",
      "entourer\n",
      "joue\n",
      "arriver\n",
      "connaître\n",
      "évanouir\n",
      "occuper\n",
      "écouler\n",
      "dormir\n",
      "arrêter\n",
      "estimer\n",
      "suffire\n",
      "frémir\n",
      "dénuer\n",
      "passer\n",
      "présenter\n",
      "endormir\n"
     ]
    }
   ],
   "source": [
    "query = \"MATCH (t: Token {pos: 'VERB'}) RETURN t\"\n",
    "result = session.execute_read(lambda tx: tx.run(query).to_eager_result()) # type: ignore\n",
    "\n",
    "print(f'{len(result.records)} verbes trouvés: ')\n",
    "for result in result.records:\n",
    "    verb = result[\"t\"]\n",
    "    print(f'{verb[\"lemma\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e87e0f8",
   "metadata": {},
   "source": [
    "Trouver les sujets d’un verbe donné : demander\n",
    "Exemple : pour le verbe “manger”, afficher les tokens reliés par :DEP {label:\"nsubj\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45a32c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sujets pour le verbe \"chercher\": ['corps']\n",
      "Sujets pour le verbe \"devoir\": ['esprit', 'malade']\n"
     ]
    }
   ],
   "source": [
    "def subjects_of_verb(verb: str) -> list[str]:\n",
    "    query = \"MATCH (t: Token {pos: 'VERB', lemma: $verb})-[:DEP {label:'nsubj'}]-(s) RETURN s.lemma as subject\"\n",
    "    result = session.execute_read(lambda tx: tx.run(query, verb=verb).to_eager_result()) # type: ignore\n",
    "    return list(map(lambda x: x[\"subject\"], result.records))\n",
    "\n",
    "print('Sujets pour le verbe \"chercher\":', subjects_of_verb(\"chercher\"))\n",
    "print('Sujets pour le verbe \"devoir\":', subjects_of_verb(\"devoir\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305017ab",
   "metadata": {},
   "source": [
    "Trouver les objets directs associés aux verbes\n",
    "→ relations :DEP {label:\"dobj\"}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4dabb87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 verbes et objets trouvés: \n",
      "chercher, sommeil\n",
      "poser, volume\n",
      "faire, réflexion\n",
      "trouver, obscurité\n",
      "relever, distance\n",
      "regarder, montre\n",
      "donne, courage\n",
      "entendre, sifflement\n",
      "entendre, craquement\n",
      "éteindre, gaz\n",
      "rester, nuit\n",
      "plonger, meuble\n",
      "dissiper, jour,--date\n",
      "naître, côte\n",
      "naître, position\n",
      "donner, entier\n",
      "voir, cité\n",
      "reculer, soleil\n",
      "ouvrir, paupière\n",
      "ouvrir, oeil\n",
      "détendre, esprit\n",
      "repérer, position\n",
      "induire, direction\n",
      "nommer, demeure\n",
      "identifier, logis\n",
      "rappeler, place\n",
      "rappeler, genre\n",
      "deviner, orientation\n",
      "devoir, cheminée\n",
      "oublier, flamme\n",
      "oublier, fille\n",
      "oublier, événement\n",
      "souffler, lumière\n",
      "prendre, tour\n",
      "parler, rivalité\n",
      "choquer, raison\n",
      "recouvrir, vue\n",
      "demander, heure\n",
      "entourer, tête\n",
      "écouler, jusqu’\n",
      "frémir, animal\n",
      "présenter, mur\n"
     ]
    }
   ],
   "source": [
    "query = \"MATCH (t: Token {pos: 'VERB'})-[:DEP {label:'obj'}]-(o) RETURN t, o\"\n",
    "result = session.execute_read(lambda tx: tx.run(query).to_eager_result()) # type: ignore\n",
    "\n",
    "print(f'{len(result.records)} verbes et objets trouvés: ')\n",
    "for result in result.records:\n",
    "    verb, object = result[\"t\"], result[\"o\"]\n",
    "    print(f'{verb[\"lemma\"]}, {object[\"lemma\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0e2ef8",
   "metadata": {},
   "source": [
    "Compter combien de fois chaque dépendance grammaticale apparaît\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33512e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('amod', 37), ('obl:mod', 43), ('nmod', 72), ('nsubj', 27), ('acl:relcl', 20), ('dep', 6), ('obj', 53), ('advcl', 30), ('xcomp', 32), ('appos', 5), ('flat:name', 4), ('conj', 34), ('advmod', 32), ('obl:arg', 33), ('acl', 16), ('case', 4), ('aux:pass', 1), ('obl:agent', 5), ('nsubj:pass', 1), ('ccomp', 2), ('fixed', 1), ('mark', 2), ('parataxis', 1)]\n",
      "23 461\n"
     ]
    }
   ],
   "source": [
    "query = \"MATCH (:Token)-[r]->() RETURN DISTINCT r.label as label, COUNT (r) as count\"\n",
    "result = session.execute_read(lambda tx: tx.run(query).to_eager_result()) # type: ignore\n",
    "res = list(map(lambda x: (x[\"label\"], x[\"count\"]), result.records))\n",
    "\n",
    "print(res)\n",
    "print(len(res), sum(map(lambda x: x[1], res), 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88991c39",
   "metadata": {},
   "source": [
    "Lister les adjectifs qui qualifient un nom donné : esprit, oiseau, train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c33afe73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjectifs qui qualifient le nom \"mur\":  ['invisible']\n",
      "Adjectifs qui qualifient le nom \"fauteuil\":  ['magique']\n",
      "Adjectifs qui qualifient le nom \"chose\":  ['incompréhensible', 'obscur']\n"
     ]
    }
   ],
   "source": [
    "def adjective_qualifying_name(noun: str) -> list[str]:\n",
    "    query = \"\"\"\n",
    "        MATCH (adj:Token{pos: 'ADJ'})-[r]-(:Token{pos: 'NOUN', lemma: $noun}) \n",
    "        RETURN adj.lemma as lemma\n",
    "    \"\"\"\n",
    "    result = session.execute_read(lambda tx: tx.run(query, noun=noun).to_eager_result())\n",
    "    return list(map(lambda x: x[\"lemma\"], result.records))\n",
    "\n",
    "nouns = [\"mur\", \"fauteuil\", \"chose\"]\n",
    "for noun in nouns:\n",
    "    print(f'Adjectifs qui qualifient le nom \"{noun}\": ', adjective_qualifying_name(noun))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce951fed",
   "metadata": {},
   "source": [
    "Mesurer les verbes les plus utilisés dans le corpus (avec une requête Cypher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b3c12e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('devoir', 12), ('venir', 11), ('oublier', 9), ('trouver', 7), ('présenter', 7), ('chercher', 7), ('naître', 7), ('imaginer', 6), ('croire', 6), ('tirer', 6)]\n"
     ]
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "        MATCH (verb:Token{pos: 'VERB'})-[r]-()\n",
    "        RETURN verb.lemma as verb, COUNT (r) as count \n",
    "        ORDER BY count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\"\n",
    "result = session.execute_read(lambda tx: tx.run(query).to_eager_result())\n",
    "res = list(map(lambda x: (x[\"verb\"], x[\"count\"]), result.records))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba86d393",
   "metadata": {},
   "source": [
    "Bonus:\n",
    "Construire des couples Sujet–Verbe–Objet\n",
    "Exemple : (\"chat\" - \"mange\" - \"souris\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d4246e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 noeuds crées et 2 liens crées\n",
      "3 noeuds crées et 2 liens crées\n",
      "3 noeuds crées et 2 liens crées\n",
      "[('corps', 'chercher', 'sommeil'), ('fauteuil', 'faire', 'réflexion'), ('Ève', 'naître', 'position'), ('femme', 'naître', 'position'), ('Ève', 'naître', 'côte'), ('femme', 'naître', 'côte'), ('sommeil', 'détendre', 'esprit'), ('lui,--mon', 'rappeler', 'place'), ('lui,--mon', 'rappeler', 'genre'), ('esprit', 'devoir', 'cheminée'), ('malade', 'devoir', 'cheminée'), ('sommeil', 'prendre', 'tour'), ('réflexion', 'prendre', 'tour'), ('église', 'parler', 'rivalité'), ('mémoire', 'présenter', 'mur'), ('chat', 'manger', 'souris'), ('musicien', 'jouer', 'violon'), ('peintre', 'peindre', 'toile')]\n"
     ]
    }
   ],
   "source": [
    "def create_sentence(subject, verb, object: str):\n",
    "    query = \"\"\"\n",
    "        MERGE (c:Token{lemma: $subject, pos: \"NOUN\"})\n",
    "        MERGE (m:Token{lemma: $verb, pos: \"VERB\"})\n",
    "        MERGE (s:Token{lemma: $object, pos: \"NOUN\"})\n",
    "        MERGE (c)-[:DEP {label: \"nsubj\"}]->(m) \n",
    "        MERGE (s)-[:DEP {label: \"obj\"}]->(m) \n",
    "    \"\"\"\n",
    "    result = session.execute_write(\n",
    "        lambda tx: tx.run(\n",
    "            query, subject=subject, verb=verb, object=object\n",
    "        ).to_eager_result()\n",
    "    )\n",
    "    print(\n",
    "        result.summary.counters.nodes_created,\n",
    "        \"noeuds crées et\",\n",
    "        result.summary.counters.relationships_created,\n",
    "        \"liens crées\",\n",
    "    )\n",
    "\n",
    "create_sentence(\"chat\", \"manger\", \"souris\")\n",
    "create_sentence(\"musicien\", \"jouer\", \"violon\")\n",
    "create_sentence(\"peintre\", \"peindre\", \"toile\")\n",
    "\n",
    "query = \"\"\"\n",
    "MATCH (subject: Token)-[:DEP {label: 'nsubj'}]->(verb:Token{pos: 'VERB'})<-[:DEP {label:'obj'}]-(object:Token)\n",
    "RETURN subject.lemma as subject, verb.lemma as verb, object.lemma as object\n",
    "\"\"\"\n",
    "result = session.execute_read(lambda tx: tx.run(query).to_eager_result())\n",
    "res = list(map(lambda x: (x[\"subject\"], x[\"verb\"], x[\"object\"]), result.records))\n",
    "\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb4a846",
   "metadata": {},
   "source": [
    "Bonus (dans l’ordre) : \n",
    "Calculez le graph de cooccurrence sous forme d’une matrice à partir du premier texte, importez ce graph dans neo4j puis mettez à jour ce graphe de cooccurrence depuis le deuxième texte (il y a des mots en commun entre ces deux paragraphes vous vous en doutez) et mettez à jour la base neo4j.\n",
    "Faites des recherches sur Neo4j Graph Data Science (GDS Library) et trouvez un algorithme de recherche de chemin le plus court, faites-en la description et expliquer comment l’utiliser avec cette librairie officielle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e00475d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80a6337",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.close()\n",
    "driver.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
